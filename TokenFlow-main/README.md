# ğŸŒ± TokenFlow â€” Optimize Prompts. Save Tokens. Protect the Planet.

**TokenFlow** is a privacy-first Chrome extension that rewrites your AI prompts *locally* before theyâ€™re sent to models like ChatGPT, Claude, or Gemini. Using **winkNLP** and semantic heuristics, it eliminates filler, sharpens clarity, and reduces token countâ€”cutting down the environmental footprint of every interaction.

> âš¡ Run faster, cost less, and do goodâ€”without changing your workflow.

---

## ğŸš€ Features

- âœ… **Automatic Prompt Optimization**  
  Silently intercepts and rewrites your prompts with smarter, cleaner phrasing.

- âœ… **Shallow Semantic Compression**  
  Uses NLP (winkNLP) and in-browser heuristics to remove redundancy while preserving intent.

- âœ… **Instant Token Savings Feedback**  
  Get real-time stats on how many tokens you saved after every prompt.

- âœ… **Sensitive Data Masking**  
  Automatically masks API keys, passwords, and secrets with `****`.

- âœ… **Sustainability Impact Tracking**  
  Tracks estimated reductions in:  
  âš¡ Electricity (kWh)  
  ğŸ’§ Water (litres)  
  ğŸŒ Carbon (gCOâ‚‚)

- âœ… **Full Privacy. Zero APIs.**  
  All processing is done 100% locally inside the browserâ€”no servers, no tracking.

- âœ… **Impact Dashboard**  
  View cumulative stats on your eco-savings over time.

---

## ğŸ“¸ Demo

Coming soon 

---

## ğŸ§  How It Works

1. **Prompt Detection**  
   Listens for prompts on ChatGPT, Claude, Gemini, etc., without altering UI.

2. **In-Browser NLP Optimization**  
   Applies winkNLP and semantic heuristics to compress and clean the prompt.

3. **Token Comparison Engine**  
   Calculates the token difference between the original and optimized prompt.

4. **Toast Notification**  
   After sending, shows token savings and environmental impact.

5. **Dashboard Display**  
   Tracks total tokens saved, water and energy conserved, and COâ‚‚ avoided.

---

## ğŸ›  Tech Stack

- **Browser**: Chrome Extension (Manifest V3)  
- **Language**: TypeScript  
- **Build Tool**: Vite  
- **UI**: Tailwind CSS v4.1 + [shadcn/ui](https://ui.shadcn.com)  
- **NLP Engine**: [winkNLP](https://winkjs.org)  
- **Storage**: Chrome Local Storage  
- **Runtime**: 100% offline, WASM-free (no model inference required)

---

## ğŸ” Privacy First

TokenFlow processes everything **locally** inside your browser.  
No prompts are sent to external servers. No tracking. No analytics.  
Your data stays yours.

---

## ğŸ“† Installation

> Chrome Web Store release coming soon!

To install manually:

```bash
# 1. Clone the repo
git clone https://github.com/Yousuf-cse/TokenFlow.git 
cd tokenflow

# 2. Install dependencies
pnpm install

# 3. Build the project
pnpm run build
```

Then:

1. Go to `chrome://extensions`
2. Enable **Developer Mode**
3. Click **Load unpacked**
4. Select the `/dist` (or `/build`) folder

---

## ğŸ§ª Use Cases

- ğŸ§‘â€ğŸ’» Developers avoiding prompt bloat in ChatGPT  
- ğŸ“µ Journalists working with sensitive data  
- ğŸ§‘â€âš– Legal researchers prioritizing privacy  
- ğŸ§‘â€ğŸ« Educators and students learning sustainable AI  
- ğŸ¢ Organizations measuring eco-impact from LLM usage  

---

## ğŸŒ Why TokenFlow?

Every LLM prompt consumes:

- âš¡ Electricity (to run models)
- ğŸ’§ Water (for data center cooling)
- ğŸŒ Carbon (indirect emissions)

Cutting just a few tokens per promptâ€”at scaleâ€”saves:

- Thousands of litres of water  
- Kilowatt-hours of electricity  
- Measurable COâ‚‚ emissions

> TokenFlow helps you reduce compute load, improve model speed, and use AI more responsibly.

---

## ğŸ§‘â€ğŸ’» Authors

Built with care by the **TokenFlow Team**  
at the *HexaFalls Hackathon*, JIS University, 2025

- Yousuf Mallik 
- Sree Gopal Saha  
- Kartik Barman  
- Sushanta Ruidas

---

## ğŸ’¡ Contribute

Have ideas or want to add support for other LLMs (e.g., Mistral, Perplexity)?  
Open an issue or a PR â€” contributions are welcome!

---

## âš  Disclaimer

TokenFlow provides **estimated** impact metrics based on public LLM energy benchmarks.  
Actual values may vary depending on model, provider, and usage patterns.

---
